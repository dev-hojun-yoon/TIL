# 25. 8. 25 (Mon)


* 머신러닝
  * 로지스틱 회귀는 시그모이드 함수 형태를 가진다.
    * 데이터 분포에 따라 결정계수를 잘 따라간다, 설명력이 있는 알고리즘
    * 과대 적합을 피하기 위해 변수를 추가함
    * 절대값의 가중치를 합해 규제(C)를 주는게 L1 규제
    * 제곱값의 가중치를 합해 규제(C)를 주는게 L2 규제
      * 가중치에 따라 다양한 편향을 가지게 되니 이런 상황에 사용한다.
  * SVM (서포트 벡터 머신)
    * 비선형 모델
    * 가장 근처에 있는 서포트 벡터 2개를 잡고 각 지점에서 마진의 폭을 최대화하는 것이 1차 목표
    * 머신러닝의 목표는 새로 들어오는 데이터에 대한 예측
    * C (규제) 를 통해 각 데이터포인트가 결정경계에 주는 영향력을 제어
    * gamma : 데이터포인트에 대한 결정경계가 미치는 영향력을 제어
    * rbf: Radial Basis Function 반경 기저 함수
    * SVC (Support Vector Classifier)
      * 데이터를 고차원 공간으로 변환해 클래스를 구분하는 초평면 찾는 알고리즘이다.
      * 초평면은 각 클래스와 가장 가까운 데이터 포인트 (서포트 벡터) 와의 거리를 최대화함.
      * 즉 커널 트릭을 사용해 선형적으로 분리할 수 없는 복잡한 데이터도 처리 가능
      * 커널 함수 (RBF, 다항식) 을 통해 데이터를 무한에 가까운 고차원 공간으로 매핑
      * 이를 통해 비선형적 경계가 새로운 차원에서 선형 분리
      * 따라서 DecisionTreeClassifier나 KNeighborsClassifier가 다루기 어려운 비선형적인 데이터셋에서 유용
  * 결정 트리
    * 지니 계수를 기본적으로 많이 사용한다.
  * 랜덤포레스트
    * 여러 개의 결정 트리 연결
    * 일반적으로 평균 0, 분산 1 기준으로 데이터를 재정의하는데, tree 계열 모델에서는 의미가 없음
    * 특성 - 특성 간 편차를 줄이는게 목적인데, tree 모델은 특성 안에서 기준을 만들기 떄문
  * k-nearest 최근접 이웃 분류 알고리즘
    * k 개는 보통 홀수개로 선정
    * 메모리에 데이터 포인트 저장한다는 특징
    * 거리 기반 알고리즘으로, scaling 영향을 받음
    * 특성 간의 편차는 거리의 차이를 크게 만들어주기 때문에 scaling 으로 성능 떨어지는 경향
  * 기타
    * 커널 변수
