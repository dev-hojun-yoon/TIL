# 25. 8. 8 (Fri)

* Kibana
  * csv 파일 import 할 때 (ML)
    * 마찬가지로 settings -> mappings 설정 내용을 확인 후 Ingest Pipeline 처리가 된다.
  * ElasticSearch Query (ES | QL) : SQL 문법과 유사하다. 1년전부터 밀고 있음
  * 쿼리를 Kibana QL 과 Lucene 모두 가능하다.
  * Stack Management -> Data views 에서 동일한 스키마의 인덱스 데이터를 합칠 수 있다.
  * Stack Management -> Saved Objects 에서 만든 데이터에 대해 export 를 할 수 있다.
* ElasticSearch
  * 인덱스를 위한 색인하기 위한 커스텀 애널라이저는 해당 인덱스 안의 settings 에 작성하고, mappings 에서 가져다 쓴다.
  * 변경을 위해서는 기존에 설정된 인덱스를 제거해야 한다.
  * 샘플
    ```
    https://www.elastic.co/docs/reference/elasticsearch/plugins/analysis-nori-speech

    PUT nori_sample
    {
      "settings": {
        "index": {
          "analysis": {
            "analyzer": {
              "my_analyzer": {
                "tokenizer": "nori_tokenizer",
                "filter": [
                  "my_posfilter"
                ]
              }
            },
            "filter": {
              "my_posfilter": {
                "type": "nori_part_of_speech",
                "stoptags": [
                  "NR"
                ]
              }
            }
          }
        }
      }
    }
    
    GET nori_sample/_analyze
    {
      "analyzer": "my_analyzer",
      "text": "여섯 용이"
    }
    ```
    ```
    PUT nori_sample
    {
      "settings": {
        "index": {
          "analysis": {
            "analyzer": {
              "my_analyzer": {
                "tokenizer": "nori_tokenizer",
                "filter": [ "nori_readingform" ]
              }
            }
          }
        }
      }
    }
    
    GET nori_sample/_analyze
    {
      "analyzer": "my_analyzer",
      "text": "鄕歌"
    }
    ```
    십만이천오백과 -> 숫자로 변경
    ３.２천 -> 3200 으로 콤마에 대한 후처리
    ```
    PUT nori_sample
    {
      "settings": {
        "index": {
          "analysis": {
            "analyzer": {
              "my_analyzer": {
                "tokenizer": "tokenizer_discard_puncuation_false",
                "filter": [
                  "part_of_speech_stop_sp", "nori_number"
                ]
              }
            },
            "tokenizer": {
              "tokenizer_discard_puncuation_false": {
                "type": "nori_tokenizer",
                "discard_punctuation": "false"
              }
            },
            "filter": {
                "part_of_speech_stop_sp": {
                    "type": "nori_part_of_speech",
                    "stoptags": ["SP"]
                }
            }
          }
        }
      }
    }
    
    GET nori_sample/_analyze
    {
      "analyzer": "my_analyzer",
      "text": "십만이천오백과 ３.２천"
    }
    ```
  * 리마인드
    * 윤동주 시인의 '별 헤는 밤' 시로 실습
    * 한글은 복합어, 합성어가 많아 하나의 단어도 여러 어간으로 분리 필요 -> 한글 형태소 사전 필요 (nori)
    * 시 내용에는 憧憬 과 같은 한자가 있음, 이를 -> 동경으로 변환해주려면 Synonym Filter 로 직접 등록 필요
    * 조사가 많이 포함되어 있으면, 찾고자 하는 키워드에 대해 점수가 낮을 수 있으니 제거해야할 필터를 잘 설정하기
    * custom 을 위해 토크나이저, 토큰 필터를 설정
      * nor_tokenizer (decompound_mode, user_dictionary)
      * 토큰 필터 (nori_part_of_speech, nori_readingform, nori_number)
      * decompound_mode 는 none 으로 세부적으로 분리되지 않도록 설정하고,
      * 의미 없는 공백은 없애도록 했습니다. "discard_punctuation": "true"
      * "type": "nori_part_of_speech"
        * 형태소 분석 후 특정 품사 (POS, part of speech) 에 해당하는 토큰을 제거하기 위해 사용
        * 조사, 기호, 감탄사 등 불필요한 품사를 제거해서 검색 정확도를 높인다.
        * type : 
      * "stoptags": ["SP"]
        * 기호 (special symbol) 를 의미하는 품사 코드
        * 기호에 해당하는 형태소를 제거하는 의미이다.
        * 따라서 쉼표, 마침표, 따옴표, 괄호 등 기호 토큰을 제거
      * 주요 POS 코드 요약
        * NNG 일반 명사 (학생 / 책)
        * NNP 고유 명사 (서울 / 윤동주)
        * VV 동사 (가다 / 보다)
        * VA 형용사 (멋진 / 큰)
        * JKS 주격 조사 (이 / 가)
        * JKB 부사격 조사 (에 / 에서)
        * SP 기호 (, . : ())
        * IC 감탄사 (와!, 아!)
        * SF 마침표 등 문장 기호 (. / ? / !)
    * 토크나이저 설정 예시
      ```
      PUT nori_poem
      {
        "settings": {
          "analysis": {
            "analyzer": { // 아래에 작성한 토크나이저, 필터를 사용한 애널라이저
              "my_nori_analyzer" : {
                "tokenizer" : "my_tokenizer",
                "filter" : [
                  "nori_readingform",
                  "synonyms_filter",  
                  "nori_posfilter"
                ]
              }
            }, 
            "tokenizer": { // 토크나이저 작성
              "my_tokenizer" :  { // 토크나이저 이름 선언
                "type" : "nori_tokenizer", 
                "decompound_mode" : "mixed",
                "user_dictionany" : "userdic_ko.txt" 
               }
            },
              "filter": { // 필터 작성
                "nori_posfilter": { // 필터 이름 선언
                  "type" : "nori_part_of_speech", 
                  "stoptags" : [ "EC", "EF", "ETM", "EP", "ETN",
                        "IC", "JKS", "JKB", "JX", "SP", "XSN"
                  ]
                },
                "synonyms_filter" : { // 필터 이름 선언 : 옛날단어를 현대 단어로 치환해서 동의어
                  "type" : "synonym",
                  "synonyms_path" : "analysis/my-synonym.txt"
                }
              }
            }
          },
          "mappings": {
            "properties" : {
              "content": { // 실제 index에서 값을 받아서 쓸 field 
                "type" : "text",
                "analyzer": "my_nori_analyzer"
              } 
            }
          }
        }
      ```

  * 기타
    * OLTP
      * 트랜잭션을 빠르게 처리하는 시스템
      * INSERT, UPDATE, DELETE
      * 정규화된 테이블로 중복을 최소화하고,
      * INSERT / UPDATE / DELETE
      * 개별행 중심
      * 밀리초 수준 빠른 응답
      * **설계 방법**
        * 정규화 (데이터 중복 제거, 무결성 보장)
        * ERD 설계 (엔티티 간 관계 -> 1:1, 1:N, M:N)
        * ACID 보장
          * 원자성 : Atomicity
          * 일관성 : Consistency
          * 독립성 : Isolation
          * 지속성 : Durability
    * OLAP
      * 데이터 분석 및 의사결정 지원
      * SELECT, GROUP BY, JOIN
      * 분석 시간 허용에 따라 OLTP 에 비해 시간이 길 수 있음 (트랜잭션이 너무 길면 서비스에 이슈가 생김)
      * BI, 매출 분석, 리포트 시스템
      * **설계 방법**
        * 반정규화 : 분석 성능 향상을 위해 데이터 중복 허용
        * ETL 설계
          * Extract : OLTP -> raw data 추출
          * Transform : 필터, 집계, 정제
          * Load : 분석 DB, DW 에 적재
      * 기술 스택 참고
        * DW : Amazon Redshift, Snowflake, Google BigQuery
        * ETL / 워크플로우 : Apache Airflow, AWS Glue, dbt, Talend
        * 처리 엔진 : Spark, Flink
        * 저장소 : S3, HDFS, Delta Lake, Iceberg
        * BI 도구 : Tableau, Power BI, Superset, Looker
        * OLAP 엔진 : Apache Druid, ClickHouse, Apache Pinot
      * ELT 구조 예시 (Airflow 기반)
        * [OLTP] : MySQL: 주문, 결제 등 트랜잭션 발생
        * [ETL] : Airflow DAG
          * MySQL → S3 또는 Data Lake 추출
          * Spark 로 집계 및 전처리
        * [DW]: Redshift, Snowflake 적재
        * [BI] Tableau, Superset 등 시각화
