# 25. 9. 2 (Tue)

* pytorch
  * 한국 튜토리얼 transform 참고
  * 활성화 함수와 최적화 단계의 차이는 무엇인지
    * 활성화는 Relu 로서 값에 따라 (0일 때는 전달하지 않고 0보다 클 때는 선형으로 전달) 가중치를 전달할 것인지를 결정하는 역할이고
    * 최적화는 최종 결정된 값에 대해 보통 시그모이드화해서 확률값으로 표시해서 역전파와 기울기 미분을 하는 과정을 거친다
    * 최적해를 알 수 없기 때문에 휴리스틱 알고리즘이라 하며, 반댓말은 deterministic 알고리즘
    * 결국에 loss 를 0으로 만드는 것이 목표 이게 optimizer 의 역할
    * 최적화는 주로 Adam 을 사용함
    * batch size 와 Adam learning rate 를 통해 loss 를 줄이는 과정이다
  * from torchsummary import summary 를 통해 모델 shape 와 param 을 알 수 있음
